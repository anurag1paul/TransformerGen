{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "\n",
    "captions_path = 'text_c10'\n",
    "img_name_path = 'images.txt'\n",
    "data_path = 'images/'\n",
    "\n",
    "word_counts = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_id = open(img_name_path).read().splitlines()\n",
    "name2id_train = {}\n",
    "id2name_train = {}\n",
    "for index, img in enumerate (name_id):\n",
    "        name2id_train[img.split(' ')[1]] = img.split(' ')[0]\n",
    "        id2name_train[img.split(' ')[0]] = img.split(' ')[1]\n",
    "\n",
    "id2caption_train = {}\n",
    "\n",
    "for name in name2id_train:\n",
    "    txt_name = '.'.join(name.split('.')[0:-1]) + '.txt'\n",
    "    txt_path = os.path.join(captions_path, txt_name)\n",
    "    idx = name2id_train[name]\n",
    "    id2caption_train[idx] = open(txt_path,  encoding='utf-8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions =[]\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "for i, captions in enumerate(id2caption_train.values()):\n",
    "    for cap in captions:\n",
    "        if len(cap) == 0:\n",
    "            continue\n",
    "        cap = cap.replace(u\"\\ufffd\\ufffd\", u\" \")\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(cap.lower())\n",
    "\n",
    "        if len(tokens) == 0:\n",
    "            print('cap', cap)\n",
    "            continue\n",
    "\n",
    "        tokens_new = []\n",
    "        for t in tokens:\n",
    "            t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "            if len(t) > 0:\n",
    "                tokens_new.append(t)\n",
    "        all_captions.append(tokens_new)\n",
    "        \n",
    "word_counts = defaultdict(float)\n",
    "captions = all_captions\n",
    "for sent in captions:\n",
    "    for word in sent:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= 0]\n",
    "\n",
    "ixtoword = {}\n",
    "ixtoword[0] = '<end>'\n",
    "wordtoix = {}\n",
    "wordtoix['<end>'] = 0\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5450\n"
     ]
    }
   ],
   "source": [
    "print(len(wordtoix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-44b70b9e6414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-44b70b9e6414>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m#                 print(images[str(i)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#                 print(lb[str(i)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        name_id = open(img_name_path).read().splitlines()\n",
    "        \n",
    "#         random.shuffle(name_id)\n",
    "        \n",
    "        name2id_train = {}\n",
    "        id2name_train = {}\n",
    "        name2id_test = {}\n",
    "        id2name_test = {}\n",
    "        for index, img in enumerate (name_id):\n",
    "            if(len(name2id_train) < 9430):\n",
    "                name2id_train[img.split(' ')[1]] = img.split(' ')[0]\n",
    "                id2name_train[img.split(' ')[0]] = img.split(' ')[1]\n",
    "            else:\n",
    "                name2id_test[img.split(' ')[1]] = img.split(' ')[0]\n",
    "                id2name_test[img.split(' ')[0]] = img.split(' ')[1]\n",
    "       \n",
    "        id2caption_train = {}\n",
    "        id2caption_test = {}\n",
    "        \n",
    "        for name in name2id_train:\n",
    "            txt_name = '.'.join(name.split('.')[0:-1]) + '.txt'\n",
    "            txt_path = os.path.join(captions_path, txt_name)\n",
    "            idx = name2id_train[name]\n",
    "            id2caption_train[idx] = open(txt_path,  encoding='utf-8').read().splitlines()\n",
    "        \n",
    "        for name in name2id_test:\n",
    "            txt_name = '.'.join(name.split('.')[0:-1]) + '.txt'\n",
    "            txt_path = os.path.join(captions_path, txt_name)\n",
    "            idx = name2id_test[name]\n",
    "            id2caption_test[idx] = open(txt_path,  encoding='utf-8').read().splitlines()\n",
    "        \n",
    "        self.name2id_train = name2id_train\n",
    "        self.id2name_train = id2name_train\n",
    "        self.name2id_test = name2id_test\n",
    "        self.id2name_test = id2name_test\n",
    "        \n",
    "        self.id2caption_train = id2caption_train\n",
    "        self.id2caption_test = id2caption_test\n",
    "        \n",
    "        self.data_transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize([256, 256]),\n",
    "#                 torchvision.transforms.CenterCrop(256),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "#         self.text_transforms = torchvision.transforms.Compose([\n",
    "# #             torchvision.transforms.ToTensor()\n",
    "#             #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#         ])\n",
    "        \n",
    "    def load_img(self, image_name):\n",
    "        image = Image.open(image_name)\n",
    "        image = self.data_transforms(image).float()\n",
    "        image = torch.autograd.Variable(image, requires_grad=False)\n",
    "        image = image.unsqueeze(0)\n",
    "        return image[0].to(device)\n",
    "    \n",
    "    def imshow(self, img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.cpu().detach().numpy()\n",
    "        plt.figure(figsize = (5,5))\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), aspect='auto')\n",
    "        \n",
    "    def padding(self, unpadded):\n",
    "        l = []\n",
    "        for i in unpadded:\n",
    "            for j in i:\n",
    "                l.append([len(j)])\n",
    "        lens = np.array(l)   \n",
    "        mask = lens[:,None] > np.arange(lens.max())\n",
    "        out = np.full(mask.shape,0)\n",
    "        out[mask] = np.concatenate(np.concatenate(unpadded))\n",
    "        out = out.reshape(len(unpadded),10,-1)\n",
    "        return torch.LongTensor(out)\n",
    "    \n",
    "    def get_data(self, train=True):\n",
    "        if train == True:\n",
    "            images = self.id2name_train\n",
    "            lb = self.id2caption_train\n",
    "        else:\n",
    "            images = self.test_id2name_test\n",
    "            lb = self.id2caption_test\n",
    "        while True:\n",
    "            ix = np.random.choice(np.arange(len(images)), self.batch_size)\n",
    "            x = []\n",
    "            z = []\n",
    "#             print(ix)\n",
    "            for i in ix:\n",
    "#                 print(images[str(i)])\n",
    "#                 print(lb[str(i)])\n",
    "                x.append(self.load_img(self.data_path + images[str(i)]))\n",
    "                            \n",
    "                tmp = []\n",
    "                for cap in lb[str(i)]:\n",
    "                    if len(cap) == 0:\n",
    "                        continue\n",
    "                    cap = cap.replace(u\"\\ufffd\\ufffd\", u\" \")\n",
    "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                    tokens = tokenizer.tokenize(cap.lower())\n",
    "\n",
    "                    if len(tokens) == 0:\n",
    "                        print('cap', cap)\n",
    "                        continue\n",
    "\n",
    "                    tokens_new = []\n",
    "                    for t in tokens:\n",
    "                        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "                        if len(t) > 0:\n",
    "                            tokens_new.append(t)\n",
    "                    tmp.append(tokens_new)\n",
    "\n",
    "                train_captions_new = []\n",
    "                for t in tmp:\n",
    "                    rev = []\n",
    "                    for w in t:\n",
    "                        if w in wordtoix:\n",
    "                            rev.append(wordtoix[w])\n",
    "                    train_captions_new.append(rev)\n",
    "                z.append(train_captions_new)\n",
    "            yield torch.stack(x), self.padding(z).to(device), list(ix)\n",
    "            \n",
    "loader = DataLoader()\n",
    "x, y, class_ids = next(loader.get_data(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "loader.imshow(x[0])\n",
    "print(y.shape)\n",
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = y[:,np.random.randint(0,10),:]\n",
    "caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_ENCODER(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
    "                 nhidden=128, nlayers=1, bidirectional=True):\n",
    "        super(RNN_ENCODER, self).__init__()\n",
    "        \n",
    "        self.ntoken = ntoken  # size of the dictionary\n",
    "        self.ninput = ninput  # size of each embedding vector\n",
    "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
    "        self.nlayers = nlayers  # Number of recurrent layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "\n",
    "        self.nhidden = nhidden // self.num_directions\n",
    "\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
    "        self.drop = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                           self.nlayers, batch_first=True,\n",
    "                           dropout=self.drop_prob,\n",
    "                           bidirectional=self.bidirectional)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.nlayers* self.num_directions, batch_size, self.nhidden).cuda(),\n",
    "                torch.zeros(self.nlayers* self.num_directions, batch_size, self.nhidden).cuda())\n",
    "\n",
    "    def forward(self, captions):\n",
    "    \n",
    "        emb = self.drop(self.encoder(captions))\n",
    "#         print(self.encoder(captions))\n",
    "        \n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        words_emb = output.transpose(1, 2)\n",
    "        \n",
    "        sent_emb = self.hidden[0].transpose(0, 1).contiguous()\n",
    "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
    "        \n",
    "        return words_emb, sent_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_ENCODER(ntoken = 5450).to(device)\n",
    "model.hidden = model.init_hidden(batch_size)\n",
    "word_emb, sent_emb = model.forward(caption)\n",
    "print(word_emb.shape, sent_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.BatchNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ENCODER(nn.Module):\n",
    "    def __init__(self, nef):\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        self.nef = 128  # define a uniform ranker\n",
    "\n",
    "        model = models.inception_v3().to(device)\n",
    "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "        model.load_state_dict(model_zoo.load_url(url))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('Load pretrained model from ', url)\n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
    "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
    "        self.Mixed_5b = model.Mixed_5b\n",
    "        self.Mixed_5c = model.Mixed_5c\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        self.Mixed_6c = model.Mixed_6c\n",
    "        self.Mixed_6d = model.Mixed_6d\n",
    "        self.Mixed_6e = model.Mixed_6e\n",
    "        self.Mixed_7a = model.Mixed_7a\n",
    "        self.Mixed_7b = model.Mixed_7b\n",
    "        self.Mixed_7c = model.Mixed_7c\n",
    "\n",
    "        self.emb_features = conv1x1(768, self.nef)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "\n",
    "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # image region features\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "        # global image features\n",
    "        cnn_code = self.emb_cnn_code(x)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)\n",
    "        return features, cnn_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = CNN_ENCODER(128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " img_features, img_sent_code = image_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_emb.shape, sent_emb.shape, img_features.shape, img_sent_code.shape)\n",
    "from torch.autograd import Variable\n",
    "label = Variable(torch.LongTensor(range(batch_size))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_attention(query, context, gamma1):\n",
    "    \"\"\"\n",
    "    query: batch x ndf x queryL\n",
    "    context: batch x ndf x ih x iw (sourceL=ihxiw)\n",
    "    mask: batch_size x sourceL\n",
    "    \"\"\"\n",
    "    batch_size, queryL = query.size(0), query.size(2)\n",
    "    ih, iw = context.size(2), context.size(3)\n",
    "    sourceL = ih * iw\n",
    "\n",
    "    # --> batch x sourceL x ndf\n",
    "    context = context.view(batch_size, -1, sourceL)\n",
    "    contextT = torch.transpose(context, 1, 2).contiguous()\n",
    "\n",
    "    # Get attention\n",
    "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
    "    # -->batch x sourceL x queryL\n",
    "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
    "    # --> batch*sourceL x queryL\n",
    "    attn = attn.view(batch_size*sourceL, queryL)\n",
    "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
    "\n",
    "    # --> batch x sourceL x queryL\n",
    "    attn = attn.view(batch_size, sourceL, queryL)\n",
    "    # --> batch*queryL x sourceL\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "    attn = attn.view(batch_size*queryL, sourceL)\n",
    "    #  Eq. (9)\n",
    "    attn = attn * gamma1\n",
    "    attn = nn.Softmax()(attn)\n",
    "    attn = attn.view(batch_size, queryL, sourceL)\n",
    "    # --> batch x sourceL x queryL\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
    "    # --> batch x ndf x queryL\n",
    "    weightedContext = torch.bmm(context, attnT)\n",
    "    \n",
    "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "\n",
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
    "\n",
    "\n",
    "def words_loss(img_features, words_emb, labels, batch_size, class_ids):\n",
    "    \"\"\"\n",
    "        words_emb(query): batch x nef x seq_len\n",
    "        img_features(context): batch x nef x 17 x 17\n",
    "    \"\"\"\n",
    "    masks = []\n",
    "    att_maps = []\n",
    "    similarities = []\n",
    "    for i in range(batch_size):\n",
    "        if class_ids is not None:\n",
    "            mask = (class_ids != class_ids[i]).astype(np.uint8)\n",
    "            masks.append(mask.reshape((1, -1)))\n",
    "                \n",
    "        words_num = word_emb.shape[2]\n",
    "        \n",
    "        # -> 1 x nef x words_num\n",
    "        word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
    "        # -> batch_size x nef x words_num\n",
    "        word = word.repeat(batch_size, 1, 1)\n",
    "        # batch x nef x 17*17\n",
    "        context = img_features\n",
    "        \"\"\"\n",
    "            word(query): batch x nef x words_num\n",
    "            context: batch x nef x 17 x 17\n",
    "            weiContext: batch x nef x words_num\n",
    "            attn: batch x words_num x 17 x 17\n",
    "        \"\"\"\n",
    "        weiContext, attn = func_attention(word, context,5)\n",
    "\n",
    "        att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
    "        # --> batch_size x words_num x nef\n",
    "        word = word.transpose(1, 2).contiguous()\n",
    "        weiContext = weiContext.transpose(1, 2).contiguous()\n",
    "        # --> batch_size*words_num x nef\n",
    "        word = word.view(batch_size * words_num, -1)\n",
    "        weiContext = weiContext.view(batch_size * words_num, -1)\n",
    "        \n",
    "        # -->batch_size*words_num\n",
    "        row_sim = cosine_similarity(word, weiContext)\n",
    "        # --> batch_size x words_num\n",
    "        row_sim = row_sim.view(batch_size, words_num)\n",
    "\n",
    "        # Eq. (10)\n",
    "        row_sim.mul_(5).exp_()\n",
    "        row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "        row_sim = torch.log(row_sim)\n",
    "\n",
    "        # --> 1 x batch_size\n",
    "        # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
    "        similarities.append(row_sim)\n",
    "\n",
    "    # batch_size x batch_size\n",
    "    similarities = torch.cat(similarities, 1)\n",
    "\n",
    "    if class_ids is not None:\n",
    "        masks = np.concatenate(masks, 0)\n",
    "        # masks: batch_size x batch_size\n",
    "        masks = torch.ByteTensor(masks).cuda()\n",
    "\n",
    "    similarities = similarities * 10\n",
    "\n",
    "    if class_ids is not None:\n",
    "        similarities.data.masked_fill_(masks, -float('inf'))\n",
    "        \n",
    "    similarities1 = similarities.transpose(0, 1)\n",
    "\n",
    "    if labels is not None:\n",
    "        loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
    "        loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
    "    else:\n",
    "        loss0, loss1 = None, None\n",
    "    return loss0, loss1, att_maps\n",
    "\n",
    "w_loss0, w_loss1, attn_maps = words_loss(img_features, word_emb, label, batch_size, class_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
